{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a33b3a-54e9-4c2e-bcd9-d2027431d420",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile requirements.txt\n",
    "numpy\n",
    "notebook\n",
    "torch==2.4.1\n",
    "torchvision==0.19.1\n",
    "torchaudio==2.4.1\n",
    "lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a8b581-05c0-4250-91b3-4442c6eca50a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -r requirements.txt   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1578d03b-d58c-4f35-b198-a0b2ad289676",
   "metadata": {},
   "source": [
    "<img src=\"graphics/E2CO_Training.png\" width=\"1000\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea62e0ed-c396-40c7-8d43-bc793c7f465b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Importing All necessary Packages\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "from typing import Optional, Tuple\n",
    "from torch import load, unique\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers.tensorboard import TensorBoardLogger\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "617f47c4-d7cf-4559-b979-0184b7119303",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Core Training Run Options\n",
    "\"\"\"\n",
    "\n",
    "args = {\n",
    "    # Core Run Options\n",
    "    'SEED': 12,\n",
    "    'model': 'miniegg_model',\n",
    "    'results_path': '\\miniegg_model\\data_files',\n",
    "    'deterministic': False,\n",
    "    'model_type': 'EGG',\n",
    "    'trail_settings': 'default',\n",
    "    'version_num': 0,\n",
    "    'log_every_n_steps': 10,\n",
    "    'enable_progress_bar': True,\n",
    "\n",
    "    # Samples Selection\n",
    "    'n_training_samples': 6000,    # 300 * 20\n",
    "    'n_validation_samples': 1000,   # 50  * 20\n",
    "    'n_testing_samples': 200,      # 10  * 20\n",
    "\n",
    "    # DataLoader Settings\n",
    "    'n_epochs': 100,\n",
    "    'batch_size_training': 12,\n",
    "    'batch_size_validation': 4,\n",
    "    'shuffled_train': True,\n",
    "    'shuffled_validation': True,\n",
    "    'num_workers': 0,\n",
    "\n",
    "    # Trainer Settings\n",
    "    'n_checkpoints2save': 10,  # n=-1 means all checkpoints, else save top n checkpoints\n",
    "    'every_n_epochs': 10,  # Set to `10` to save the model every 10 epochs\n",
    "    'max_n_gpus': 1,\n",
    "    'mixed_precision': False,\n",
    "\n",
    "    # Model Information\n",
    "    'nW': 12,  # Injectors = 8 | Producers = 4 \n",
    "    'nObs': 16,  # Injectors+=1 | Producers+=2\n",
    "\n",
    "    # Latent Settings\n",
    "    'l_z': 50,\n",
    "\n",
    "    # Encoder Network Settings\n",
    "    'nEncLinear': 3,\n",
    "\n",
    "    # Decoder Network Settings\n",
    "    'nDecLinear': 3,\n",
    "\n",
    "    # Transition Network Settings\n",
    "    'nTrans': 200,\n",
    "    'nTransfBlock': 2,\n",
    "\n",
    "    # Transition Outputs Network Settings\n",
    "    'nTransWD': 20,\n",
    "    'nTransfBlockWD': 2,\n",
    "\n",
    "    # Loss Function Scaling (Regularization)\n",
    "    'well_data_loss_scale': 1,  # \n",
    "    'weight_trans_reg': 0.001,  # 0.008296379118348702,\n",
    "    'weight_trans_reg1': 0.001,  # 0.005617127794698825,\n",
    "    'weight_reg_ABCD': 0.001,  # 0.002038408624236886,\n",
    "    'latent_loss_scale': 0.001,\n",
    "\n",
    "    # Scheduler and Optimizer\n",
    "    'use_scheduler': True,\n",
    "    'scheduler_step_size': 200,\n",
    "    'scheduler_gamma': 0.5,\n",
    "    'adam_learning_rate': 1e-3,\n",
    "    'weight_decay': 0,  # 0.001\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54c691f-9a1d-4b27-93ed-f6b9a2b431ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Dataset class: Read training data --> __getitem__ does mini-batching for all data sets.\n",
    "\"\"\"\n",
    "\n",
    "class ResDataset(Dataset):\n",
    "    def __init__(self, data_type, n_samples, data_files_folder):\n",
    "\n",
    "        self.input_state = load(os.path.join(data_files_folder, data_type, \"input_state.pt\"))\n",
    "        self.output_state = load(os.path.join(data_files_folder, data_type, \"output_state.pt\"))\n",
    "        self.input_delta_t = load(os.path.join(data_files_folder, data_type, \"input_delta_t.pt\"))\n",
    "        self.input_controls = load(os.path.join(data_files_folder, data_type, \"input_controls.pt\"))\n",
    "        self.output_welldata = load(os.path.join(data_files_folder, data_type, \"output_welldata.pt\"))\n",
    "\n",
    "        self.n_samples = n_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"Returns the number of samples.\"\"\"\n",
    "        return self.n_samples\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        data = {\n",
    "            'x': self.input_state[idx, :, :],                         # x_t = [1, nodes, features]\n",
    "            'x_tp1': self.output_state[idx, :, :],                    # x_t+1 = [1, nodes, features]\n",
    "            'delta_t': self.input_delta_t[idx, :],                    # dt = [1, 1]\n",
    "            'u': self.input_controls[idx, :],                         # u = [1, nW]\n",
    "            'y_tp1': self.output_welldata[idx, :],                    # y_t+1 [1, nC]\n",
    "        }\n",
    "\n",
    "        return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e34e9fe-523a-405f-b9ff-4dd58ceac21c",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data Module: Interface for accessing dataset class during training\n",
    "\"\"\"\n",
    "\n",
    "class ResDataModule(pl.LightningModule):\n",
    "    def __init__(self, args, aux_data):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "\n",
    "        self.data_files_folder = aux_data[\"data_files_folder\"]\n",
    "        self.edge_indecies = aux_data[\"edge_indecies\"]\n",
    "\n",
    "    def prepare_data(self):\n",
    "        pass\n",
    "\n",
    "    def setup(self, stage: str = None):\n",
    "\n",
    "        # Assign train/val datasets for use in dataloaders\n",
    "        if stage == 'fit' or stage is None:\n",
    "            self.train_data = ResDataset(data_type=\"training\", \n",
    "                                         n_samples=self.args[\"n_training_samples\"], \n",
    "                                         data_files_folder=self.data_files_folder)\n",
    "\n",
    "            self.val_data = ResDataset(data_type=\"validation\", \n",
    "                                       n_samples=self.args[\"n_validation_samples\"], \n",
    "                                       data_files_folder=self.data_files_folder)\n",
    "\n",
    "        if stage == 'validate':\n",
    "            self.val_data = ResDataset(data_type=\"validation\", \n",
    "                                       n_samples=self.args[\"n_validation_samples\"], \n",
    "                                       data_files_folder=self.data_files_folder)\n",
    "\n",
    "        if stage == 'test':\n",
    "            self.test_data = ResDataset(data_type=\"testing\", \n",
    "                                        n_samples=self.args[\"n_testing_samples\"], \n",
    "                                        data_files_folder=self.data_files_folder)\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        return DataLoader(self.train_data, \n",
    "                          batch_size=self.args[\"batch_size_training\"], \n",
    "                          shuffle=self.args[\"shuffled_train\"], \n",
    "                          num_workers=self.args[\"num_workers\"])\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        return DataLoader(self.val_data, \n",
    "                          batch_size=self.args[\"batch_size_validation\"], \n",
    "                          shuffle=self.args[\"shuffled_validation\"], \n",
    "                          num_workers=self.args[\"num_workers\"])\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        return DataLoader(self.test_data, \n",
    "                          batch_size=self.args[\"n_testing_samples\"], \n",
    "                          num_workers=self.args[\"num_workers\"])\n",
    "    \n",
    "    def get_edge_indecies(self):\n",
    "        return self.edge_indecies\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6092dcc8-1bec-4af3-9ed1-42a321df7b15",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Relative Perm Class\n",
    "\"\"\"\n",
    "\n",
    "class RelPerm():\n",
    "    def __init__(self, a, b,\n",
    "                 kr_w_max, s_w_crit, s_o_irr, \n",
    "                 kr_o_max, s_w_con, s_o_r,\n",
    "                 visc_o, visc_w):\n",
    "        # Fitting parameters for the relative permeability curves\n",
    "        self.a = a\n",
    "        self.b = b\n",
    "\n",
    "        # Maximum relative permeabilities and saturation endpoints\n",
    "        self.kr_w_max = kr_w_max\n",
    "        self.s_w_crit = s_w_crit\n",
    "        self.s_o_irr = s_o_irr\n",
    "        self.kr_o_max = kr_o_max\n",
    "        self.s_w_con = s_w_con\n",
    "        self.s_o_r = s_o_r\n",
    "\n",
    "        # Viscosities\n",
    "        self.visc_o = visc_o\n",
    "        self.visc_w = visc_w\n",
    "    \n",
    "    def send_to_device(self, device):\n",
    "        self.a = self.a.to(device)\n",
    "        self.b = self.b.to(device)\n",
    "        self.kr_w_max = self.kr_w_max.to(device)\n",
    "        self.s_w_crit = self.s_w_crit.to(device)\n",
    "        self.s_o_irr = self.s_o_irr.to(device)\n",
    "        self.kr_o_max = self.kr_o_max.to(device)\n",
    "        self.s_w_con = self.s_w_con.to(device)\n",
    "        self.s_o_r = self.s_o_r.to(device)\n",
    "        self.visc_o = self.visc_o.to(device)\n",
    "        self.visc_w = self.visc_w.to(device)\n",
    "        return self\n",
    "\n",
    "    def func_kro(self, sw, a):\n",
    "        # Oil relative permeability function.\n",
    "        return self.kr_o_max * torch.pow(((1.0 - sw - self.s_o_r) / (1.0 - self.s_w_con - self.s_o_r)), a)\n",
    "\n",
    "    def func_krw(self, sw, b):\n",
    "        # Water relative permeability function.\n",
    "        return self.kr_w_max * torch.pow(((sw - self.s_w_crit) / (1.0 - self.s_w_crit - self.s_o_irr)), b)\n",
    "\n",
    "    def k_ro(self, S_w):\n",
    "        # Compute oil relative permeability with saturation clipping.\n",
    "        S_w = torch.clamp(S_w, self.s_w_con, 1.0 - self.s_o_r)\n",
    "        return self.func_kro(S_w, self.a)\n",
    "\n",
    "    def k_rw(self, S_w):\n",
    "        # Compute water relative permeability with saturation clipping.\n",
    "        S_w = torch.clamp(S_w, self.s_w_crit + 0.001, 1.0 - self.s_o_irr)\n",
    "        return self.func_krw(S_w, self.b)\n",
    "\n",
    "    def k_ro_tf(self, S_w):\n",
    "        # Compute oil relative permeability in half precision.\n",
    "        S_w = torch.clamp(S_w, self.s_w_con, 1.0 - self.s_o_r)\n",
    "        a_fp16 = self.a.to(torch.float16)\n",
    "        return self.func_kro(S_w.to(torch.float16), a_fp16)\n",
    "\n",
    "    def k_rw_tf(self, S_w):\n",
    "        # Compute water relative permeability in half precision.\n",
    "        S_w = torch.clamp(S_w, self.s_w_crit + 0.001, 1.0 - self.s_o_irr)\n",
    "        b_fp16 = self.b.to(torch.float16)\n",
    "        return self.func_krw(S_w.to(torch.float16), b_fp16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87bcd355-3f14-41c9-b3f1-51cfbd4da16a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Part of the \"Secret Sauce\"\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def broadcast(src: torch.Tensor, other: torch.Tensor, dim: int):\n",
    "    if dim < 0:\n",
    "        dim = other.dim() + dim\n",
    "    if src.dim() == 1:\n",
    "        for _ in range(0, dim):\n",
    "            src = src.unsqueeze(0)\n",
    "    for _ in range(src.dim(), other.dim()):\n",
    "        src = src.unsqueeze(-1)\n",
    "    src = src.expand(other.size())\n",
    "    return src\n",
    "\n",
    "def scatter_sum(src: torch.Tensor,\n",
    "                index: torch.Tensor,\n",
    "                dim: int = -1,\n",
    "                out: Optional[torch.Tensor] = None,\n",
    "                dim_size: Optional[int] = None) -> torch.Tensor:\n",
    "    index = broadcast(index, src, dim)\n",
    "    if out is None:\n",
    "        size = list(src.size())\n",
    "        if dim_size is not None:\n",
    "            size[dim] = dim_size\n",
    "        elif index.numel() == 0:\n",
    "            size[dim] = 0\n",
    "        else:\n",
    "            size[dim] = int(index.max()) + 1\n",
    "        out = torch.zeros(size, dtype=src.dtype, device=src.device)\n",
    "        return out.scatter_add_(dim, index, src)\n",
    "    else:\n",
    "        return out.scatter_add_(dim, index, src)\n",
    "\n",
    "def scatter_mean(src: torch.Tensor,\n",
    "                 index: torch.Tensor,\n",
    "                 dim: int = -1,\n",
    "                 out: Optional[torch.Tensor] = None,\n",
    "                 dim_size: Optional[int] = None) -> torch.Tensor:\n",
    "    out = scatter_sum(src, index, dim, out, dim_size)\n",
    "    dim_size = out.size(dim)\n",
    "\n",
    "    index_dim = dim\n",
    "    if index_dim < 0:\n",
    "        index_dim = index_dim + src.dim()\n",
    "    if index.dim() <= index_dim:\n",
    "        index_dim = index.dim() - 1\n",
    "\n",
    "    ones = torch.ones(index.size(), dtype=src.dtype, device=src.device)\n",
    "    count = scatter_sum(ones, index, index_dim, None, dim_size)\n",
    "    count[count < 1] = 1\n",
    "    count = broadcast(count, out, dim)\n",
    "    if out.is_floating_point():\n",
    "        out.true_divide_(count)\n",
    "    else:\n",
    "        out.div_(count, rounding_mode='floor')\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d0d578-7dbc-459d-ba59-137a46b8efb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Encoder Module: Part of the \"Secret Sauce\"\n",
    "\"\"\"\n",
    "\n",
    "def batch_offset_enc(edge_index, B, N, M):\n",
    "    \"\"\"\n",
    "    Given edge_index [2*E] for one graph, returns flattened src/dst indices\n",
    "    for B graphs (each with N fine nodes, M coarse nodes) after you do x.view(B*N,…).\n",
    "    \"\"\"\n",
    "    b = torch.arange(B, device=edge_index.device)[:, None]   # (B,1)\n",
    "    ei0 = (edge_index[0] + b * M).view(-1)                   # (B*E,)\n",
    "    ei1 = (edge_index[1] + b * N).view(-1)                   # (B*E,)\n",
    "    return ei0.long(), ei1.long()\n",
    "\n",
    "class FineToCoarseConv(torch.nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_pool_edges, num_coarse, custom_weight=None):\n",
    "        super().__init__()\n",
    "        self.num_coarse = num_coarse\n",
    "        self.linear = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        if custom_weight is None:\n",
    "            self.edge_weight = nn.Parameter(torch.empty(num_pool_edges, 1))\n",
    "        else:\n",
    "            self.edge_weight = nn.Parameter(custom_weight.unsqueeze(1))\n",
    "\n",
    "        self.init_weights(custom_weight)\n",
    "\n",
    "    def forward(self, x_fine, edge_index, B):\n",
    "\n",
    "        x_fine = self.linear(x_fine)\n",
    "\n",
    "        ei0, ei1 = batch_offset_enc(edge_index, B, x_fine.size(0)/B, self.num_coarse)\n",
    "\n",
    "        weights_repeated = self.edge_weight.repeat(B, 1).contiguous()\n",
    "        messages = x_fine[ei1] * weights_repeated\n",
    "\n",
    "        x_coarse = scatter_mean(messages, ei0, dim=0, dim_size= B*self.num_coarse).contiguous()\n",
    "        \n",
    "        return x_coarse\n",
    "    \n",
    "    def init_weights(self, custom_weight):\n",
    "        if custom_weight is None:\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "            nn.init.xavier_uniform_(self.edge_weight)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "\n",
    "class EncoderModule(torch.nn.Module):\n",
    "    def __init__(self, args, aux_data):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.aux_data = aux_data\n",
    "\n",
    "        self.edges = aux_data[\"edge_indecies\"]\n",
    "        self.nEncLinear = args[\"nEncLinear\"]\n",
    "        self.edges_custom = aux_data[\"perm_edges\"]\n",
    "\n",
    "        self.conv1 = FineToCoarseConv(  2,  32, num_pool_edges=aux_data[\"num_pool_edges\"][0], num_coarse=aux_data[\"num_coarse\"][0])\n",
    "        self.conv2 = FineToCoarseConv( 32,  64, num_pool_edges=aux_data[\"num_pool_edges\"][1], num_coarse=aux_data[\"num_coarse\"][1])\n",
    "        self.conv3 = FineToCoarseConv( 64, 128, num_pool_edges=aux_data[\"num_pool_edges\"][2], num_coarse=aux_data[\"num_coarse\"][2])\n",
    "        self.conv4 = FineToCoarseConv(128, 128, num_pool_edges=aux_data[\"num_pool_edges\"][3], num_coarse=aux_data[\"num_coarse\"][3])\n",
    "        self.conv5 = FineToCoarseConv(128, 128, num_pool_edges=aux_data[\"num_pool_edges\"][4], num_coarse=aux_data[\"num_coarse\"][4])\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(32)\n",
    "        self.ln2 = nn.LayerNorm(64)\n",
    "        self.ln3 = nn.LayerNorm(128)\n",
    "        self.ln4 = nn.LayerNorm(128)\n",
    "        self.ln5 = nn.LayerNorm(128)\n",
    "\n",
    "        self.silu = torch.nn.SiLU()\n",
    "\n",
    "        self.final_linear = nn.Linear(aux_data[\"num_coarse\"][-1] * 128, self.args[\"l_z\"])\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def stack_graphs_enc(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        B, N, F = x.size()\n",
    "        out = x.new_empty(B * N, F)\n",
    "        for i in range(B):\n",
    "            out[i * N : (i + 1) * N] = x[i]\n",
    "        return out\n",
    "    \n",
    "    def unstack_flat_enc(self, x: torch.Tensor, B: int, N: int, F: int) -> torch.Tensor:\n",
    "        out = x.new_empty(B, N * F)\n",
    "        for i in range(B):\n",
    "            block = x[i * N : (i + 1) * N]       # shape [N, F]\n",
    "            out[i] = block.reshape(N * F)        # shape [N*F]\n",
    "        return out\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        edges = self.edges\n",
    "\n",
    "        B = len(x)\n",
    "        x = x.contiguous()\n",
    "        x = self.stack_graphs_enc(x).to(x)\n",
    "\n",
    "        x = self.conv1(x, edges[0].to(x), B)\n",
    "        x = self.ln1(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.conv2(x, edges[1].to(x), B)\n",
    "        x = self.ln2(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.conv3(x, edges[2].to(x), B)\n",
    "        x = self.ln3(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.conv4(x, edges[3].to(x), B)\n",
    "        x = self.ln4(x)\n",
    "        x = self.silu(x)\n",
    "        x = self.conv5(x, edges[4].to(x), B)\n",
    "        x = self.ln5(x)\n",
    "        x = self.silu(x)\n",
    "\n",
    "        x = x.contiguous()\n",
    "        x = self.unstack_flat_enc(x, B=B, N=self.aux_data[\"num_coarse\"][-1], F=128).to(x)\n",
    "\n",
    "        x = self.final_linear(x)\n",
    "\n",
    "        return x\n",
    "    \n",
    "    def init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.final_linear.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd94b9bc-4fe6-444a-b5e9-3f652298f4a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Decoder Module: Part of the \"Secret Sauce\"\n",
    "\"\"\"\n",
    "\n",
    "def batch_offset_dec(edge_index, B, N, M):\n",
    "    \"\"\"\n",
    "    Given edge_index [2*E] for one graph, returns flattened src/dst indices\n",
    "    for B graphs (each with N fine nodes, M coarse nodes) after you do x.view(B*N,…).\n",
    "    \"\"\"\n",
    "    b = torch.arange(B, device=edge_index.device)[:, None]   # (B,1)\n",
    "    ei0 = (edge_index[0] + b * N).view(-1)                   # (B*E,)\n",
    "    ei1 = (edge_index[1] + b * M).view(-1)                   # (B*E,)\n",
    "    return ei0.long(), ei1.long()\n",
    "\n",
    "class CoarseToFineConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_pool_edges, num_fine, non_relu_init=False):\n",
    "        super().__init__()\n",
    "        self.num_fine = num_fine\n",
    "        self.linear = nn.Linear(in_channels, out_channels, bias=False)\n",
    "        self.edge_weight = nn.Parameter(torch.empty(num_pool_edges, 1))\n",
    "\n",
    "        self.init_weights(non_relu_init)\n",
    "\n",
    "    def forward(self, x_coarse, edge_index, B):\n",
    "\n",
    "        x_coarse = self.linear(x_coarse)\n",
    "\n",
    "        ei0, ei1 = batch_offset_dec(edge_index, B, x_coarse.size(0)/B, self.num_fine)\n",
    "\n",
    "        weights_repeated = self.edge_weight.repeat(B, 1).contiguous()\n",
    "        messages = x_coarse[ei0] * weights_repeated\n",
    "\n",
    "        x_fine = scatter_mean(messages, ei1, dim=0, dim_size=(self.num_fine)*B).contiguous()\n",
    "        \n",
    "        return x_fine\n",
    "    \n",
    "    def init_weights(self, non_relu_init):\n",
    "        if non_relu_init:\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "            nn.init.xavier_uniform_(self.edge_weight)\n",
    "        else:\n",
    "            nn.init.xavier_uniform_(self.linear.weight)\n",
    "            nn.init.xavier_uniform_(self.edge_weight)\n",
    "\n",
    "\n",
    "class DecoderModule(torch.nn.Module):\n",
    "    def __init__(self, args, aux_data):\n",
    "        super().__init__()\n",
    "        self.args = args\n",
    "        self.aux_data = aux_data\n",
    "\n",
    "        self.edges = aux_data[\"edge_indecies\"]\n",
    "        self.nDecLinear = args[\"nDecLinear\"]\n",
    "\n",
    "        self.conv5 = CoarseToFineConv(128, 128, num_pool_edges=aux_data[\"num_pool_edges\"][4],  num_fine=aux_data[\"num_fine\"][4])\n",
    "        self.conv4 = CoarseToFineConv(128, 128, num_pool_edges=aux_data[\"num_pool_edges\"][3],  num_fine=aux_data[\"num_fine\"][3])\n",
    "        self.conv3 = CoarseToFineConv(128,  64, num_pool_edges=aux_data[\"num_pool_edges\"][2],  num_fine=aux_data[\"num_fine\"][2])\n",
    "        self.conv2 = CoarseToFineConv( 64,  32, num_pool_edges=aux_data[\"num_pool_edges\"][1],  num_fine=aux_data[\"num_fine\"][1])\n",
    "        self.conv1 = CoarseToFineConv( 32,   2, num_pool_edges=aux_data[\"num_pool_edges\"][0],  num_fine=aux_data[\"num_fine\"][0]\n",
    "                                      , non_relu_init=True)\n",
    "\n",
    "        self.ln5 = nn.LayerNorm(128)\n",
    "        self.ln4 = nn.LayerNorm(128)\n",
    "        self.ln3 = nn.LayerNorm(64)\n",
    "        self.ln2 = nn.LayerNorm(32)\n",
    "        self.ln1 = nn.LayerNorm(2)\n",
    "\n",
    "        self.silu = torch.nn.SiLU()\n",
    "\n",
    "        self.initial_linear = nn.Linear(self.args[\"l_z\"], aux_data[\"num_coarse\"][-1] * 128)\n",
    "\n",
    "        self.init_weights()\n",
    "    \n",
    "    def unstack_graphs_dec(self, x: torch.Tensor, B: int, N: int) -> torch.Tensor:\n",
    "        F = x.size(1)\n",
    "        out = x.new_empty(B, N, F)\n",
    "        for i in range(B):\n",
    "            out[i] = x[i * N : (i + 1) * N]\n",
    "        return out\n",
    "    \n",
    "    def stack_flat_dec(self, x: torch.Tensor, B: int, N: int, F: int) -> torch.Tensor:\n",
    "        return x.reshape(B, N, F).reshape(B * N, F)\n",
    "    \n",
    "    def forward(self, z):\n",
    "\n",
    "        edges = self.edges\n",
    "\n",
    "        z = self.initial_linear(z)\n",
    "        z = self.silu(z)\n",
    "\n",
    "        B = len(z)\n",
    "\n",
    "        z = z.contiguous()\n",
    "        z = self.stack_flat_dec(z, B=B, N=self.aux_data[\"num_coarse\"][-1], F=128).to(z)\n",
    "\n",
    "        z = self.conv5(z, edges[4].to(z), B)\n",
    "        z = self.ln5(z)\n",
    "        z = self.silu(z)\n",
    "        z = self.conv4(z, edges[3].to(z), B)\n",
    "        z = self.ln4(z)\n",
    "        z = self.silu(z)\n",
    "        z = self.conv3(z, edges[2].to(z), B)\n",
    "        z = self.ln3(z)\n",
    "        z = self.silu(z)\n",
    "        z = self.conv2(z, edges[1].to(z), B)\n",
    "        z = self.ln2(z)\n",
    "        z = self.silu(z)\n",
    "        z = self.conv1(z, edges[0].to(z), B)\n",
    "\n",
    "        z = z.contiguous()\n",
    "        z = self.unstack_graphs_dec(z, B=B, N=self.aux_data[\"num_fine\"][0]).to(z)\n",
    "        z = torch.sigmoid(5.0 * (z - 0.5))*1.15\n",
    "\n",
    "        return z\n",
    "    \n",
    "    def init_weights(self):        \n",
    "        nn.init.xavier_uniform_(self.initial_linear.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fed52b4-c534-4cf0-9403-47f749ede893",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transition Module: Core \"E2CO\" concept\n",
    "\"\"\"\n",
    "\n",
    "class TransitionModule(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(TransitionModule, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.nW = args[\"nW\"]\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.l_z = args[\"l_z\"]\n",
    "        self.nTrans = args[\"nTrans\"]\n",
    "        self.nTransfBlock = args[\"nTransfBlock\"]\n",
    "\n",
    "        # Layers\n",
    "        self.linear_layers = nn.ModuleList([\n",
    "            nn.Linear(self.l_z + 1 if i == 0 else self.nTrans, self.nTrans) \n",
    "            for i in range(self.nTransfBlock)\n",
    "        ])\n",
    "        self.linear_z_A = nn.Linear(self.nTrans, self.l_z  * self.l_z)\n",
    "        self.linear_z_B = nn.Linear(self.nTrans, self.l_z  * self.nW)\n",
    "\n",
    "        # Regularization Activations\n",
    "        self.z_A_activations = None\n",
    "        self.z_B_activations = None\n",
    "\n",
    "        # Weight Initialization\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, z, delta_t, u):\n",
    "        # Concat time to latent state\n",
    "        z_del_t = torch.cat((z, delta_t), dim=1).to(z)\n",
    "\n",
    "        # Transformation Structure\n",
    "        for i in range(self.nTransfBlock):\n",
    "            z_del_t = self.linear_layers[i](z_del_t)\n",
    "            z_del_t = torch.relu(z_del_t)\n",
    "\n",
    "        # Step Matrices calculation (flat)\n",
    "        z_A = self.linear_z_A(z_del_t)\n",
    "        z_B = self.linear_z_B(z_del_t)\n",
    "\n",
    "        # Activity Regularization Equivalent\n",
    "        self.z_A_activations = z_A.abs().sum()\n",
    "        self.z_B_activations = z_B.abs().sum()\n",
    "\n",
    "        # Step Matrices reshape\n",
    "        z_A = z_A.view(-1, self.l_z, self.l_z).to(z)\n",
    "        z_B = z_B.view(-1, self.l_z, self.nW).to(z)\n",
    "\n",
    "        # Ensuing control batch size match\n",
    "        u = u.view(-1, self.nW).to(z)\n",
    "\n",
    "        # MOR Computations\n",
    "        u_dt = torch.mul(delta_t, u).to(z)\n",
    "        sysmodel = torch.bmm(z_A, z.unsqueeze(-1)).squeeze(-1).to(z)\n",
    "        syscontrol = torch.bmm(z_B, u_dt.unsqueeze(-1)).squeeze(-1).to(z)\n",
    "        z_tp1_hat = torch.add(sysmodel, syscontrol).to(z)\n",
    "\n",
    "        return z_tp1_hat\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for linear_layer in self.linear_layers:\n",
    "            nn.init.xavier_uniform_(linear_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_z_A.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_z_B.weight)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57745b0e-e87c-48e6-a309-36f93912ba13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Transition Output Module: Core \"E2CO\" concept\n",
    "\"\"\"\n",
    "\n",
    "class TransitionOutputModule(pl.LightningModule):\n",
    "    def __init__(self, args):\n",
    "        super(TransitionOutputModule, self).__init__()\n",
    "\n",
    "        # Parameters\n",
    "        self.nW = args[\"nW\"]\n",
    "        self.nObs = args[\"nObs\"]\n",
    "\n",
    "        # Hyperparameters\n",
    "        self.l_z = args[\"l_z\"]\n",
    "        self.nTransWD = args[\"nTransWD\"]\n",
    "        self.nTransfBlockWD = args[\"nTransfBlockWD\"]\n",
    "\n",
    "        # Layers\n",
    "        self.linear_layers = nn.ModuleList([\n",
    "            nn.Linear(self.l_z + 1 if i == 0 else self.nTransWD, self.nTransWD) \n",
    "            for i in range(self.nTransfBlockWD)\n",
    "        ])\n",
    "        self.linear_z_C = nn.Linear(self.nTransWD, self.nObs * self.l_z)\n",
    "        self.linear_z_D = nn.Linear(self.nTransWD, self.nObs * self.nW)\n",
    "\n",
    "        # Regularization Activations\n",
    "        self.z_C_activations = None\n",
    "        self.z_D_activations = None\n",
    "\n",
    "        # Weight Initialization\n",
    "        self.initialize_weights()\n",
    "\n",
    "    def forward(self, z, delta_t, u, z_tp1_hat):\n",
    "        # Concat time to latent state\n",
    "        z_del_t = torch.cat((z, delta_t), dim=1).to(z)\n",
    "        \n",
    "        # Transformation Structure\n",
    "        for i in range(self.nTransfBlockWD):\n",
    "            z_del_t = self.linear_layers[i](z_del_t)\n",
    "            z_del_t = torch.relu(z_del_t)\n",
    "\n",
    "        # Step Matrices calculation (flat)\n",
    "        z_C = self.linear_z_C(z_del_t)\n",
    "        z_D = self.linear_z_D(z_del_t)\n",
    "\n",
    "        # Activity Regularization Equivalent\n",
    "        self.z_C_activations = z_C.abs().sum()\n",
    "        self.z_D_activations = z_D.abs().sum()\n",
    "\n",
    "        # Step Matrices reshape\n",
    "        z_C = z_C.view(-1, self.nObs, self.l_z).to(z)\n",
    "        z_D = z_D.view(-1, self.nObs, self.nW).to(z)\n",
    "\n",
    "        # Ensuing control batch size match\n",
    "        u = u.view(-1, self.nW).to(z)\n",
    "\n",
    "        # MOR Computations\n",
    "        u_dt = torch.mul(delta_t, u).to(z)\n",
    "        sysmodel = torch.bmm(z_C, z_tp1_hat.unsqueeze(-1)).squeeze(-1).to(z)\n",
    "        syscontrol = torch.bmm(z_D, u_dt.unsqueeze(-1)).squeeze(-1).to(z)\n",
    "        y_tp1_hat = torch.add(sysmodel, syscontrol).to(z)\n",
    "\n",
    "        return y_tp1_hat\n",
    "\n",
    "    def initialize_weights(self):\n",
    "        for linear_layer in self.linear_layers:\n",
    "            nn.init.xavier_uniform_(linear_layer.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_z_C.weight)\n",
    "        nn.init.xavier_uniform_(self.linear_z_D.weight)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a236c994-82bd-4300-8b93-bb8fbdae3b44",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Custom flexible wrapper for dynamic loss type selection\n",
    "\"\"\"\n",
    "\n",
    "class CustomLosses(nn.Module):\n",
    "    \"\"\"\n",
    "        # Defining Losses\n",
    "        self.euclid_loss = euclidean_loss()\n",
    "        self.euclid_loss_mask = euclidean_loss_mask()\n",
    "        self.l1_reg_loss = l1_reg_loss()\n",
    "        self.loss_phys_grad_1ph = loss_flux_singlePhase()\n",
    "        self.loss_phys_grad_2ph = loss_flux_twoPhase()\n",
    "        self.well_data_loss = well_data_loss()  # might need to do the hack here as well\n",
    "        self.prod_sat_qw_loss = loss_prod_sat_qw()\n",
    "        self.loss_qw_neg = loss_qw_neg()\n",
    "        self.loss_wells_gridblock = loss_wells_gridblock()\n",
    "    \"\"\"\n",
    "    def __init__(self, device, norms):\n",
    "        super(CustomLosses, self).__init__()\n",
    "        # Device\n",
    "        self.device = device\n",
    "\n",
    "        # Store norms for normalization\n",
    "        self.norms = norms\n",
    "\n",
    "        # Dictionary mapping loss type names to functions\n",
    "        self.loss_functions = {\n",
    "            'mse': self.mse_loss,\n",
    "            'l1': self.l1_loss,\n",
    "            'euclid': self.euclid_loss,\n",
    "            'latent': self.latent_loss,\n",
    "            'l1_reg_loss': self.l1_reg_loss,\n",
    "            'l1_reg_loss_dual': self.l1_reg_loss_dual,\n",
    "            'l2_norm_loss': self.l2_norm_loss,\n",
    "            'l2_norm_loss_sa': self.l2_norm_loss_sa,\n",
    "            'l1_l2_mix_loss': self.l1_l2_mix_loss,\n",
    "            #'flux_loss': self.flux_loss,\n",
    "        }\n",
    "\n",
    "    def mse_loss(self, x, y, reduction='sum', **kwargs):\n",
    "\n",
    "        loss = nn.MSELoss(reduction=reduction)\n",
    "\n",
    "        return loss(x, y)\n",
    "    \n",
    "    def l1_loss(self, x, y, reduction='sum', **kwargs):\n",
    "\n",
    "        loss = torch.nn.L1Loss(reduction=reduction)\n",
    "\n",
    "        return loss(x, y)\n",
    "    \n",
    "    def euclid_loss(self, x, y):\n",
    "        \n",
    "        diff_square = torch.pow(torch.flatten(y) - torch.flatten(x), 2)\n",
    "        loss = torch.sum(torch.sqrt(torch.sum(diff_square, dim=-1)))\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def l2_norm_loss(self, x, y):\n",
    "\n",
    "        return torch.norm(x - y, p=2)\n",
    "    \n",
    "    def l2_norm_loss_sa(self, x, y, sa_w):\n",
    "        s = sa_w\n",
    "        \n",
    "        norm_p = torch.sum(torch.sqrt(torch.sum(torch.square(x[:, :, 0] - y[:, :, 0]), dim=1))).contiguous()\n",
    "        norm_s = torch.sum(torch.sqrt(torch.sum(torch.square(s * (x[:, :, 1] - y[:, :, 1])), dim=1))).contiguous()\n",
    "\n",
    "        norm = norm_p + norm_s\n",
    "\n",
    "        return norm\n",
    "    \n",
    "    def l1_l2_mix_loss(self, x, y):\n",
    "        \n",
    "        xp, xs = torch.split(x, 1, dim=-1)\n",
    "        yp, ys = torch.split(y, 1, dim=-1)\n",
    "        \n",
    "        p_loss = self.l2_norm_loss(xp, yp)\n",
    "        s_loss = self.l1_loss(xs, ys)\n",
    "\n",
    "        return p_loss + s_loss\n",
    "    \n",
    "    def latent_loss(self, x, **kwargs):\n",
    "\n",
    "        loss = torch.sum(x ** 2)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def l1_reg_loss(self, qm, **kwargs):\n",
    "\n",
    "        qm = qm.view(qm.size(0), -1).contiguous()\n",
    "        loss = torch.norm(qm, p=1, dim=-1)\n",
    "        loss = torch.sum(loss)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def l1_reg_loss_dual(self, x, y, **kwargs):\n",
    "\n",
    "        loss = (self.l1_reg_loss(x) + self.l1_reg_loss(y)) / 2.0\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def forward(self, x1, y1=None, x2=None, y2=None, loss_type='mse', **kwargs):\n",
    "        \"\"\" \n",
    "        Args:\n",
    "            x (torch.Tensor): Model predictions.\n",
    "            y (torch.Tensor): Ground truth values.\n",
    "            loss_type (str): The type of loss to compute (e.g., 'mse', 'l1').\n",
    "        \n",
    "        Returns:\n",
    "            torch.Tensor: The computed loss.\n",
    "        \"\"\"\n",
    "        if loss_type not in self.loss_functions:\n",
    "            raise ValueError(f\"Unsupported loss type: {loss_type}\")\n",
    "        \n",
    "        loss_fn = self.loss_functions[loss_type]\n",
    "        # Check if targets is provided. If not, call loss function with outputs only.\n",
    "        if y1 is not None and x2 is not None and y2 is not None:\n",
    "            return loss_fn(x1, y1, x2, y2, **kwargs)\n",
    "        elif y1 is not None and x2 is not None:\n",
    "            return loss_fn(x1, y1, x2, **kwargs)\n",
    "        elif y1 is not None:\n",
    "            return loss_fn(x1, y1, **kwargs)\n",
    "        else:\n",
    "            return loss_fn(x1, **kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a03acd-6e5b-4acf-a58c-7c40b4e68f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Main Lightning class: Let's put it all together!\n",
    "\"\"\"\n",
    "\n",
    "class LightningE2CO(pl.LightningModule):\n",
    "\n",
    "    def __init__(self, args, aux_data):\n",
    "        super(LightningE2CO, self).__init__()\n",
    "        # Base Parameters\n",
    "        self.args = args\n",
    "        self.aux_data = aux_data\n",
    "\n",
    "        # Defining Networks\n",
    "        self.Encoder = EncoderModule(self.args, self.aux_data)\n",
    "        self.Decoder = DecoderModule(self.args, self.aux_data)\n",
    "        self.Transition = TransitionModule(self.args)\n",
    "        self.TransitionOutput = TransitionOutputModule(self.args)\n",
    "\n",
    "        # Defining Loss Class\n",
    "        self.loss_module = CustomLosses(device=self.device, norms=self.aux_data['norms'])\n",
    "\n",
    "        # Model outputs for validations\n",
    "        self.test_outputs = []\n",
    "\n",
    "        # SA-Weights Stuff\n",
    "        self.automatic_optimization = True  # Enable manual optimization\n",
    "        self.sa_w = None\n",
    "\n",
    "    def forward(self, batch):  # Need to add another input (Settings)\n",
    "\n",
    "        # Unpack\n",
    "        x_t, x_tp1, delta_t, u, y_tp1 = batch.values()\n",
    "        batch_size = torch.tensor(len(x_t))\n",
    "\n",
    "        # Prediction -> Calculation of x_tp1_hat\n",
    "        z_t = self.Encoder(x=x_t)\n",
    "\n",
    "        # Transition -> Calculation of z_tp1_hat\n",
    "        z_tp1_hat = self.Transition(z=z_t, delta_t=delta_t, u=u)\n",
    "\n",
    "        x_t_hat = self.Decoder(z=z_tp1_hat)\n",
    "\n",
    "        # Calculation of z_tp1\n",
    "        z_tp1 = self.Encoder(x=x_tp1)\n",
    "\n",
    "        # Reconstruction -> Calculation of x_tp1_hat and x_t_hat\n",
    "        x_tp1_hat = self.Decoder(z=z_tp1)\n",
    "\n",
    "        # Calculation of y_tp1_hat\n",
    "        y_tp1_hat = self.TransitionOutput(z=z_t, delta_t=delta_t, u=u, z_tp1_hat=z_tp1)\n",
    "\n",
    "        # y_tp1_hat = None\n",
    "\n",
    "        return (x_t, x_t_hat, x_tp1, x_tp1_hat, z_t, z_tp1, z_tp1_hat, y_tp1, y_tp1_hat, batch_size)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "\n",
    "        # Main forward Step (Entire network)\n",
    "        batch = self.forward(batch)\n",
    "\n",
    "        # Calculate Losses\n",
    "        losses = self.calculate_losses(batch, batch_idx, self.sa_w)\n",
    "\n",
    "        # Log Losses\n",
    "        for loss_name, loss_value in losses.items():\n",
    "            self.log(f'{loss_name}_Train', loss_value, prog_bar=True)\n",
    "\n",
    "        return losses['Loss_Total']\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "\n",
    "        # Main forward Step (Entire network)\n",
    "        batch = self.forward(batch)\n",
    "\n",
    "        # Calculate Losses\n",
    "        losses = self.calculate_losses(batch, batch_idx, self.sa_w)\n",
    "\n",
    "        # Log Losses\n",
    "        for loss_name, loss_value in losses.items():\n",
    "            self.log(f'{loss_name}_Validation', loss_value, prog_bar=True)\n",
    "\n",
    "        return losses['Loss_Total']\n",
    "\n",
    "    def calculate_losses(self, batch, batch_idx, sa_weights):\n",
    "\n",
    "        x_t, x_t_hat, x_tp1, x_tp1_hat, z_t, z_tp1, z_tp1_hat, y_tp1, y_tp1_hat, batch_size = batch\n",
    "\n",
    "        # The Main Losses\n",
    "        loss_reconstruction = self.loss_module(x_t_hat, x_t, loss_type='l2_norm_loss')\n",
    "        loss_prediction = self.loss_module(x_tp1_hat, x_tp1, loss_type='l2_norm_loss')\n",
    "        loss_transition = self.loss_module(z_tp1, z_tp1_hat, loss_type='l2_norm_loss')\n",
    "\n",
    "        # Well Data Loss\n",
    "        loss_well_data = self.loss_module(y_tp1_hat, y_tp1, loss_type='l2_norm_loss')  # * self.args['well_data_loss_scale']\n",
    "\n",
    "        # Latent Space minimization loss\n",
    "        loss_latent = self.args['latent_loss_scale'] * self.loss_module(z_t, loss_type='latent')\n",
    "\n",
    "        # Combined Latent Space (Activity Regularization) minimization loss (ABCD)\n",
    "        if self.args['weight_reg_ABCD'] > 0:\n",
    "            loss_ABCD = self.Transition.z_A_activations + \\\n",
    "                        self.Transition.z_B_activations + \\\n",
    "                        self.TransitionOutput.z_C_activations + \\\n",
    "                        self.TransitionOutput.z_D_activations\n",
    "            loss_ABCD = loss_ABCD * self.args['weight_reg_ABCD']\n",
    "        else:\n",
    "            loss_ABCD = 0\n",
    "\n",
    "        # Transition Regularization Loss\n",
    "        if self.args['weight_trans_reg'] > 0:\n",
    "            loss_trans_reg = self.loss_module(z_t, loss_type='l1_reg_loss') * self.args['weight_trans_reg']\n",
    "        else:\n",
    "            loss_trans_reg = 0\n",
    "\n",
    "        # Transition Regularization Loss 1\n",
    "        if self.args['weight_trans_reg1'] > 0:\n",
    "            loss_trans_reg1 = self.loss_module(z_tp1_hat, z_tp1, loss_type='l1_reg_loss_dual') * self.args['weight_trans_reg1']\n",
    "        else:\n",
    "            loss_trans_reg1 = 0\n",
    "\n",
    "        # Computing Total Loss\n",
    "        loss_total = loss_reconstruction + loss_prediction + loss_transition + \\\n",
    "            loss_latent + loss_ABCD + loss_trans_reg + loss_trans_reg1 + loss_well_data\n",
    "        \n",
    "        # # Computing Total Loss\n",
    "        # loss_NON_Reg = loss_reconstruction + loss_prediction + loss_transition\n",
    "                                \n",
    "        # Normalizaation and Packaging\n",
    "        losses = {\n",
    "            'Reconstruction': loss_reconstruction / batch_size,\n",
    "            'Prediction': loss_prediction / batch_size,\n",
    "            'Transition': loss_transition / batch_size,\n",
    "            'Well': loss_well_data / batch_size,\n",
    "            'Latent': loss_latent / batch_size,\n",
    "            'ABCD': loss_ABCD / batch_size,\n",
    "            'TransReg': loss_trans_reg / batch_size,\n",
    "            'TransReg1': loss_trans_reg1 / batch_size,\n",
    "            # 'Flux': loss_flux / batch_size,\n",
    "            'Loss_Total': loss_total / batch_size,\n",
    "            # 'Loss_NON_Reg': loss_NON_Reg / batch_size,\n",
    "        }\n",
    "\n",
    "        return losses\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "\n",
    "        x_t, x_tp1, delta_t, u, y_tp1 = batch.values()\n",
    "        batch_size = torch.tensor(len(x_t))\n",
    "\n",
    "        samples = int(x_t.shape[0]/20)\n",
    "        nodes = int(x_t.shape[1])\n",
    "        times_steps = 20\n",
    "        features = 2\n",
    "\n",
    "        x_t = x_t.view(samples, times_steps, nodes, features)\n",
    "        x_tp1 = x_tp1.view(samples, times_steps, 18553, features)\n",
    "        u = u.view(samples, times_steps, 12)\n",
    "        y_tp1 = y_tp1.view(samples, times_steps, 16)\n",
    "        delta_t = delta_t.view(samples, times_steps, 1)\n",
    "\n",
    "        pred_x_tp1_hat = torch.empty(samples, times_steps+1, nodes, features).to(self.device)\n",
    "        pred_z_t_hat = torch.empty(samples, times_steps+1, self.args[\"l_z\"]).to(self.device)  # lz change here\n",
    "        pred_y_tp1_hat = torch.empty(samples, times_steps, 16).to(self.device)\n",
    "        orig_y_tp1_hat = y_tp1.to(self.device)\n",
    "        orig_x_tp1_hat = torch.cat([x_t[:, 0:1, :, :], x_tp1], dim=1).to(self.device)\n",
    "\n",
    "        for sample in range(samples):\n",
    "            initial_state = x_t[sample, 0:1, :, :]\n",
    "            z_t_hat = self.Encoder(x=initial_state)\n",
    "            pred_z_t_hat[sample, 0, :] = z_t_hat\n",
    "            pred_x_tp1_hat[sample, 0, :, :] = initial_state\n",
    "\n",
    "            for time_step in range(1, times_steps+1):\n",
    "                u_in = u[sample, time_step-1:time_step, :]\n",
    "                delta_t_in = delta_t[sample, time_step-1:time_step, :]\n",
    "                z_tp1_hat = self.Transition(z=pred_z_t_hat[sample, time_step-1:time_step, :], delta_t=delta_t_in, u=u_in)\n",
    "                pred_z_t_hat[sample, time_step, :] = z_tp1_hat\n",
    "\n",
    "                y_tp1_hat = self.TransitionOutput(z=pred_z_t_hat[sample, time_step-1:time_step, :], delta_t=delta_t_in, u=u_in, z_tp1_hat=z_tp1_hat)\n",
    "                pred_y_tp1_hat[sample, time_step-1, :] = y_tp1_hat\n",
    "\n",
    "                x_tp1_hat = self.Decoder(z=z_tp1_hat)\n",
    "                pred_x_tp1_hat[sample, time_step, :, :] = x_tp1_hat\n",
    "        \n",
    "        results = (pred_x_tp1_hat, orig_x_tp1_hat, pred_y_tp1_hat, orig_y_tp1_hat, pred_z_t_hat)\n",
    "\n",
    "        self.test_outputs = results\n",
    "\n",
    "        return\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        params = list(self.Encoder.parameters()) + list(self.Decoder.parameters()) + list(self.Transition.parameters())\n",
    "        optimizer = torch.optim.Adam(params, lr=self.args['adam_learning_rate'], weight_decay=self.args['weight_decay'])\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=self.args['scheduler_step_size'], gamma=self.args['scheduler_gamma'])\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "    def on_test_epoch_end(self):\n",
    "        torch.save(self.test_outputs, rf\"{self.args['results_path']}\\test_outputs.pt\")\n",
    "        return self.test_outputs\n",
    "    \n",
    "    def on_train_batch_end(self, batch, batch_idx, dataloader_idx=None):\n",
    "        lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "        self.log(\"lr\", lr, prog_bar=True)\n",
    "    \n",
    "    # def on_train_epoch_end(self):\n",
    "    #     lr = self.trainer.optimizers[0].param_groups[0]['lr']\n",
    "    #     print(f\"Epoch {self.current_epoch} Done | LR={lr}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7caf5009-2cd6-4b0a-905b-645fc7ee42d0",
   "metadata": {},
   "source": [
    "<img src=\"graphics/E2CO_infer.png\" width=\"800\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f82ab1ad-dbf8-4ac4-a556-fddff3c93f5c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Training\n",
    "\"\"\"\n",
    "\n",
    "# Filter Future Warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "# SEED\n",
    "pl.seed_everything(args['SEED'])\n",
    "\n",
    "# Initial Settings\n",
    "model_name = 'miniegg_model'\n",
    "current_directory = os.getcwd()\n",
    "data_files_folder = os.path.join(current_directory, model_name, 'data_files', 'tensor_data')\n",
    "\n",
    "# Loading Auxillary Data\n",
    "# Coarsening Edge Indecies\n",
    "edge_indecies = []\n",
    "num_pool_edges = []\n",
    "num_coarse = []\n",
    "num_fine = []\n",
    "for i in [1, 2, 3, 4, 5]:\n",
    "    vals = load(os.path.join(data_files_folder, 'common', f'edge{i}.pt'))\n",
    "    edge_indecies.append(vals)\n",
    "    num_pool_edges.append(vals.size(dim=1))\n",
    "    num_coarse.append(unique(vals[0, :]).size(dim=0))\n",
    "    num_fine.append(unique(vals[1, :]).size(dim=0))\n",
    "    \n",
    "# Original Edge Index\n",
    "original_edge = load(os.path.join(data_files_folder, 'testing', 'edge_index.pt')).permute(1, 0)\n",
    "\n",
    "# Node Coordinates\n",
    "node_coords = load(os.path.join(data_files_folder, 'common', 'node_coords.pt'))\n",
    "# node_coords = None\n",
    "    \n",
    "# Permability Field Data\n",
    "perm = load(os.path.join(data_files_folder, 'common', 'permi.pt'))\n",
    "perm_normalized = (perm - perm.min()) / (perm.max() - perm.min())\n",
    "perm_activations = perm_normalized[edge_indecies[0][1]]\n",
    "perm_edges = perm[original_edge[0][1]]\n",
    "\n",
    "# Relative Permeability Data\n",
    "krel_data = load(os.path.join(data_files_folder, 'common', 'krel_data.pt'))\n",
    "krel = RelPerm(\n",
    "    a=krel_data['a'],\n",
    "    b=krel_data['b'],\n",
    "    kr_w_max=krel_data['kr_w_max'],\n",
    "    s_w_crit=krel_data['s_w_crit'],\n",
    "    s_o_irr=krel_data['s_o_irr'],\n",
    "    kr_o_max=krel_data['kr_o_max'],\n",
    "    s_w_con=krel_data['s_w_con'],\n",
    "    s_o_r=krel_data['s_o_r'],\n",
    "    visc_o=krel_data['visc_o'],\n",
    "    visc_w=krel_data['visc_w'],\n",
    ")\n",
    "# krel = None\n",
    "\n",
    "# Normalizations\n",
    "norms = load(os.path.join(data_files_folder, 'common', 'norms.pt'))\n",
    "\n",
    "aux_DATALOADER_data = {\n",
    "    \"edge_indecies\": edge_indecies,\n",
    "    \"data_files_folder\": data_files_folder,\n",
    "}\n",
    "\n",
    "aux_MODEL_data = {\n",
    "    \"perm\": perm,\n",
    "    \"perm_activations\": perm_activations,\n",
    "    \"krel\": krel,\n",
    "    \"norms\": norms,\n",
    "    \"node_coords\": node_coords,\n",
    "    \"original_edge\": original_edge,\n",
    "    \"perm_edges\": perm_edges,\n",
    "    \"num_pool_edges\" : num_pool_edges,\n",
    "    \"num_coarse\" : num_coarse,\n",
    "    \"num_fine\" : num_fine,\n",
    "    \"edge_indecies\": edge_indecies,\n",
    "}\n",
    "\n",
    "# Data Loader Init\n",
    "res_training_data = ResDataModule(args, aux_DATALOADER_data)\n",
    "\n",
    "# Model Init\n",
    "# ckpt_path=os.path.join(current_directory, model_name, \"checkpoints\", \"EGG_LAST\", \"s30_lr1e3\", \"0\", \"epoch=498.ckpt\")\n",
    "# model = LightningE2CO.load_from_checkpoint(ckpt_path, args=args, aux_data=aux_MODEL_data)\n",
    "model = LightningE2CO(args=args, aux_data=aux_MODEL_data)  # Initialize model\n",
    "\n",
    "# Setting up checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    dirpath=os.path.join(current_directory, model_name, 'checkpoints', args['model_type'], args['trail_settings'], str(args['version_num'])),\n",
    "    filename='{epoch}',  # Filename will include the epoch\n",
    "    save_top_k=args[\"n_checkpoints2save\"],\n",
    "    monitor=\"Loss_Total_Validation\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# Setting up TensorBoard logger\n",
    "logger_dir = os.path.join(current_directory, model_name, f\"log_{args['model_type']}\")\n",
    "logger = TensorBoardLogger(logger_dir, name=f\"{args['model_type']}_{args['trail_settings']}_v{args['version_num']}\")\n",
    "\n",
    "trainer = Trainer(\n",
    "    max_epochs=args[\"n_epochs\"],\n",
    "    accelerator=\"gpu\", devices=\"auto\",\n",
    "    callbacks=checkpoint_callback,\n",
    "    logger=logger,\n",
    "    deterministic=args[\"deterministic\"],\n",
    "    enable_progress_bar=args[\"enable_progress_bar\"],\n",
    "    log_every_n_steps=args[\"log_every_n_steps\"],\n",
    ")\n",
    "\n",
    "trainer.fit(model, datamodule=res_training_data)  # Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4e18cc-c325-49df-89b1-27ae000ad9ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Run Tensorboard Logger for monitoring\n",
    "\"\"\"\n",
    "\n",
    "!tensorboard --logdir=miniegg_model/log_EGG"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
